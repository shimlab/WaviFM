{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to enable this notebook to import from libraries\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..\\..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from scripts.mockUtilities import *\n",
    "from scripts.utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import pywt\n",
    "import pywt.data\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.io import mmread\n",
    "from sklearn.mixture import GaussianMixture\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = mmread(r'..\\..\\data\\10x_visium_LIBD\\sample9\\counts_hvg.mtx')\n",
    "counts_sct = mmread(r'..\\..\\data\\10x_visium_LIBD\\sample9\\sct_transformed_counts.mtx')\n",
    "coords = pd.read_csv(r'..\\..\\data\\10x_visium_LIBD\\sample9\\coords.csv')\n",
    "meta_data = pd.read_csv(r'..\\..\\data\\10x_visium_LIBD\\sample9\\meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = coords.rename(columns={'x_coord': 'x', 'y_coord': 'y'})\n",
    "\n",
    "# counts data\n",
    "cell_feature_ori = counts\n",
    "cell_feature_ori = (cell_feature_ori.toarray())\n",
    "meta_data[\"feature_ori\"] =  [cell_feature_ori[:,x] for x in range(meta_data.shape[0])] \n",
    "\n",
    "# sctransformed counts data\n",
    "cell_feature_sct = counts_sct\n",
    "cell_feature_sct = (cell_feature_sct.toarray())\n",
    "meta_data[\"feature_sct\"] =  [cell_feature_sct[:,x] for x in range(meta_data.shape[0])] \n",
    "\n",
    "meta_data = meta_data[['barcode', 'feature_sct', 'feature_ori', 'spatialLIBD']]\n",
    "rna = pd.concat([coords, meta_data], axis=1)\n",
    "rna['original index'] = rna.index\n",
    "rna = rna.dropna()\n",
    "rna['spatialLIBDCode'] = rna['spatialLIBD'].astype('category').cat.codes\n",
    "rna = rna.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data\n",
    "\n",
    "We will be using the feature_sct data (sctransformed data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the settings for the data\n",
    "N_GENES = len(rna[\"feature_sct\"][0])\n",
    "\n",
    "# Set the settings for the gridding of the data\n",
    "MIN_X = 130\n",
    "MAX_X = 500\n",
    "MIN_Y = -510\n",
    "MAX_Y = -100\n",
    "N_X_INDICES = 2**6\n",
    "N_Y_INDICES = N_X_INDICES\n",
    "N_CELLS = N_X_INDICES * N_Y_INDICES\n",
    "\n",
    "# Calculate the size of each cell in the grid (these are boxes in the thesis)\n",
    "cell_size_x = (MAX_X - MIN_X) / N_X_INDICES\n",
    "cell_size_y = (MAX_Y - MIN_Y) / N_Y_INDICES\n",
    "\n",
    "# Assign each (x, y) point to a grid cell\n",
    "rna['grid_x'] = ((rna['x'] - MIN_X) // cell_size_x).astype(int)\n",
    "rna['grid_y'] = ((rna['y'] - MIN_Y) // cell_size_y).astype(int)\n",
    "\n",
    "# Group by grid cells (grid_x, grid_y) and calculate the average feature vector for each cell\n",
    "grouped = rna.groupby(['grid_y', 'grid_x'])['feature_sct'].apply(lambda x: np.mean(np.vstack(x), axis=0))\n",
    "fill_feature_vector = np.zeros(N_GENES)\n",
    "full_index = pd.MultiIndex.from_product([range(N_Y_INDICES), range(N_X_INDICES)], names=['y_index', 'x_index'])\n",
    "grouped_reindexed = grouped.reindex(full_index, fill_value=fill_feature_vector)\n",
    "grid_rna = pd.DataFrame(grouped_reindexed.tolist(), index=grouped_reindexed.index)\n",
    "\n",
    "# Rename the columns to gene_indicator_n where n is the element index of the feature vector\n",
    "grid_rna.columns = [f'gene_indicator_{i}' for i in range(grid_rna.shape[1])]\n",
    "\n",
    "# Reset index to have y_index and x_index as named columns\n",
    "grid_rna.index.names = ['y_index', 'x_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rna_unnormalised = grid_rna.copy()\n",
    "result_rna_unnormalised['feature'] = result_rna_unnormalised.apply(lambda row: row.tolist(), axis=1)\n",
    "result_rna_unnormalised = result_rna_unnormalised.drop(result_rna_unnormalised.columns[:-1], axis=1)\n",
    "result_rna_unnormalised = result_rna_unnormalised.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform standardization \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "result_rna = result_rna_unnormalised.copy()\n",
    "matrix = result_rna_unnormalised['feature'].tolist()\n",
    "matrix = np.array(matrix)\n",
    "scaler = StandardScaler()\n",
    "matrix_standardized = scaler.fit_transform(matrix)\n",
    "standardized_features = matrix_standardized.tolist()\n",
    "result_rna['feature'] = standardized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(matrix_standardized.mean(axis=0),np.array([0.0]*N_GENES))\n",
    "assert np.allclose(matrix_standardized.std(axis=0),np.array([1.0]*N_GENES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FACTORS = 7 # M in thesis\n",
    "N_LENGTH_SCALES = 4 # D in thesis\n",
    "n_resolutions = N_LENGTH_SCALES + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = N_FACTORS\n",
    "feats = np.vstack(result_rna['feature'].to_numpy())\n",
    "pca = PCA(n_components=n_components)\n",
    "flattened_L_pca = pca.fit_transform(feats)\n",
    "F_pca = pca.components_ # F matrix, factor vs feature\n",
    "\n",
    "L_pca = [np.zeros((N_Y_INDICES, N_X_INDICES)) for _ in range(n_components)]\n",
    "for i, row in result_rna.iterrows():\n",
    "    y_idx = row['y_index']\n",
    "    x_idx = row['x_index']\n",
    "    \n",
    "    for component_index in range(n_components):\n",
    "        L_pca[component_index][y_idx, x_idx] = flattened_L_pca[i, component_index]\n",
    "\n",
    "LF_pca = pca.inverse_transform(flattened_L_pca).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run WaviFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scripts.cavi_plot_utilities import *\n",
    "from scripts.cavi import *\n",
    "import copy\n",
    "from scripts.cavi_utilities import *\n",
    "from scripts.cavi_evaluation import *\n",
    "from scripts.utilities import *\n",
    "from scripts.run_wavifm import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = N_FACTORS\n",
    "p_eta_shape = (n_factors,)\n",
    "p_pi_shape = (n_resolutions,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup priors (mainly for imposing strong levels of sparsity for finer resolutions)\n",
    "priors = {\n",
    "    \"log_p_pi\": np.log(np.array((31/32,31/32,1/4,1/8,1/16))).astype(np.float64),\n",
    "    \"log_p_eta\": np.log(np.full(p_eta_shape, 0.2).astype(np.float64)),\n",
    "}\n",
    "assert priors[\"log_p_pi\"].shape == p_pi_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation 1:\n",
      "\tELBO = -10462413.902255729\n",
      "\t#Iterations = 53\n",
      "\tTime taken (s) = 768.9727478027344\n",
      "Initialisation 2:\n",
      "\tELBO = -10476976.900444908\n",
      "\t#Iterations = 395\n",
      "\tTime taken (s) = 5674.423114299774\n",
      "Initialisation 3:\n",
      "\tELBO = -10493054.249996228\n",
      "\t#Iterations = 422\n",
      "\tTime taken (s) = 6106.735743522644\n",
      "Initialisation 4:\n",
      "\tELBO = -10489329.9289578\n",
      "\t#Iterations = 395\n",
      "\tTime taken (s) = 5645.811079025269\n",
      "Initialisation 5:\n",
      "\tELBO = -10456269.869452242\n",
      "\t#Iterations = 101\n",
      "\tTime taken (s) = 1494.4556975364685\n",
      "Initialisation 5 has maximal ELBO and is returned\n"
     ]
    }
   ],
   "source": [
    "results = run_wavifm(\n",
    "    result_rna=result_rna,\n",
    "    n_length_scales=N_LENGTH_SCALES,\n",
    "    n_factors=N_FACTORS,\n",
    "    n_x_indices=N_X_INDICES,\n",
    "    n_y_indices=N_Y_INDICES,\n",
    "    max_iterations=1000,\n",
    "    relative_elbo_threshold=0.00001,\n",
    "    n_init=5,\n",
    "    priors=priors\n",
    ")\n",
    "param_results = results[\"parameters\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_means = variational_approx_posterior_mean_L(param_results)\n",
    "F_means = variational_approx_posterior_mean_F(param_results)\n",
    "pi_means = variational_approx_posterior_mean_pi(param_results)\n",
    "eta_means = variational_approx_posterior_mean_eta(param_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get factor activities in spatial domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_flattened_matrix(flattened_matrix):\n",
    "    flattened_matrix_arr = np.array(flattened_matrix)\n",
    "    n = int(np.sqrt(len(flattened_matrix_arr)))\n",
    "    square_matrix = flattened_matrix_arr.reshape((n, n))\n",
    "    return square_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_means_formatted = copy.deepcopy(L_means)\n",
    "n_factors, n_features = np.array(param_results[\"mu_F\"]).shape\n",
    "mean_values = copy.deepcopy(param_results[\"mu_L\"])\n",
    "\n",
    "for l in range(n_factors):\n",
    "    for i in range(len(param_results[\"mu_L\"][l])):\n",
    "        for j in range(len(param_results[\"mu_L\"][l][i])):\n",
    "            L_means_formatted[l][i][j] = square_flattened_matrix(L_means[l][i][j])\n",
    "\n",
    "for l in range(n_factors):\n",
    "    L_means_formatted[l][0] = square_flattened_matrix(L_means[l][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor 0\n",
      "factor 1\n",
      "factor 2\n",
      "factor 3\n",
      "factor 4\n",
      "factor 5\n",
      "factor 6\n"
     ]
    }
   ],
   "source": [
    "# Inverse Wavelet Transform on the factor loadings\n",
    "idwt_L_means = [None]*n_factors\n",
    "\n",
    "for l in range(n_factors):\n",
    "    print(f\"factor {l}\")\n",
    "    idwt_L_means[l] = pywt.waverec2(L_means_formatted[l], 'haar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export inferred factor activities\n",
    "np.savez('L_pca.npz', *L_pca)\n",
    "np.savez('idwt_L_means.npz', *idwt_L_means)\n",
    "np.savez('F_means.npz', *F_means)\n",
    "np.savez('F_pca.npz', *F_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'py3.7.8'",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
